{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfbe81-d358-434a-9f69-8e64666265e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95fb2477-a87e-4fde-a312-40f3398cb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем путь в которой работаем\n",
    "main_path = os.getcwd()\n",
    "\n",
    "# Загружаем необходимые для работы файлы\n",
    "logs = pd.read_csv(main_path+'\\\\logs.csv')\n",
    "genres = pd.read_csv(main_path+'\\\\genres.csv')\n",
    "movies = pd.read_csv(main_path+'\\\\movies.csv')\n",
    "staff = pd.read_csv(main_path+'\\\\staff.csv')\n",
    "countries = pd.read_csv(main_path+'\\\\countries.csv')\n",
    "\n",
    "# str -> list\n",
    "movies['genres'] = movies['genres'].apply(eval)\n",
    "movies['countries'] = movies['countries'].apply(eval)\n",
    "movies['staff'] = movies['staff'].apply(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eba05a6-07ba-4776-b28f-d2e6e6a28001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.5 kB ? eta -:--:--\n",
      "     ---------------------- ----------------- 30.7/55.5 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 55.5/55.5 kB 580.0 kB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.5/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.6/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.4 MB/s eta 0:00:00\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "     ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.6/8.2 MB 8.6 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.0/8.2 MB 10.7 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 1.4/8.2 MB 10.2 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.0/8.2 MB 10.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 2.5/8.2 MB 9.9 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 3.0/8.2 MB 10.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.5/8.2 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 4.0/8.2 MB 10.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.5/8.2 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 5.1/8.2 MB 10.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.6/8.2 MB 10.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 6.2/8.2 MB 10.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.7/8.2 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 7.2/8.2 MB 11.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 7.8/8.2 MB 11.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.2/8.2 MB 10.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.2/8.2 MB 10.5 MB/s eta 0:00:00\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\pyproject\\venv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/a8/01/18232f93672c1d530834e2e0568a80eaab1df12d67ae499b1762ab462b5c/regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\pyproject\\venv\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\pyproject\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.5/269.5 kB 8.4 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13775 sha256=f5bad29e8da1eeded06093338df62d5c22516cab8596a981047a8c6ea8669323\n",
      "  Stored in directory: c:\\users\\пользователь\\appdata\\local\\pip\\cache\\wheels\\1a\\b0\\8c\\4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, regex, pymorphy2, click, nltk\n",
      "Successfully installed click-8.1.7 dawg-python-0.7.2 docopt-0.6.2 nltk-3.8.1 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 regex-2023.12.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2 nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "nltk.download('punkt')\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1284f272-4ad8-4dc6-bd29-e07db19274b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyLemmatizer(texts:tuple):\n",
    "  morph = MorphAnalyzer()\n",
    "\n",
    "  def prepare(text: str):\n",
    "      prep_text = text.lower()\n",
    "      prep_text = re.sub(r'\\b\\w\\b', '', prep_text)\n",
    "      prep_text = re.sub(r'\\b\\w\\w\\b', '', prep_text)\n",
    "      prep_text = re.sub(r'[^\\w\\s]', ' ', prep_text)\n",
    "      prep_text = (i for i in prep_text.split() if i != '')\n",
    "      prep_text = (i for i in prep_text if i not in stopwords)\n",
    "      return tuple(prep_text)\n",
    "\n",
    "  old_word = prepare(' '.join(texts))\n",
    "\n",
    "  norm_word = (\n",
    "      max(((r.score, r.normal_form) for r in morph.parse(word)), key=lambda x: x[0])[1]\n",
    "      for word in old_word\n",
    "  )\n",
    "\n",
    "  norm_dict = dict(zip(old_word, norm_word))\n",
    "  word_dict = dict(Counter(old_word))\n",
    "  max_count = max(word_dict.values())\n",
    "  word_dict = {word: count / max_count for word, count in word_dict.items()}\n",
    "\n",
    "  result = [\n",
    "      (np.mean([word_dict.get(norm_dict.get(word, None), 0.0) for word in prepare(text)]))\n",
    "      if prepare(text)\n",
    "      else 0.0\n",
    "      for text in texts\n",
    "  ]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243dbfe6-237b-4924-91a0-5b162f0a8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLemmatizer:\n",
    "  def __init__(self):\n",
    "    self.norm_dict = {}\n",
    "    self.word_dict = {}\n",
    "    self.morph = MorphAnalyzer()\n",
    "\n",
    "  def prepare(self, text:str):\n",
    "    prep_text = text.lower()\n",
    "    prep_text = re.sub(r'\\b\\w\\w\\b', '', prep_text)\n",
    "    prep_text = re.sub(r'\\b\\w\\b', '', prep_text)\n",
    "    prep_text = re.sub(r'[^\\w\\s]', ' ', prep_text)\n",
    "    prep_text = [i for i in prep_text.split(' ') if i!='']\n",
    "    prep_text = [i for i in prep_text if i not in stopwords]\n",
    "    return prep_text\n",
    "\n",
    "  def fit(self, texts:list):\n",
    "    word_list = []\n",
    "    for word in self.prepare(' '.join(texts)):\n",
    "      m_score, m_words = zip(*((r.score, r.normal_form) for r in self.morph.parse(word)))\n",
    "      max_score_index = max(range(len(m_score)), key=m_score.__getitem__)\n",
    "      norm_word = m_words[max_score_index]\n",
    "      word_list.append(norm_word)\n",
    "      self.norm_dict[word] = norm_word\n",
    "\n",
    "    c = Counter(word_list)\n",
    "    self.word_dict = {k:v/max(c.values()) for k, v in c.items()}\n",
    "\n",
    "  def transform(self, text:str):\n",
    "    lem_text = []\n",
    "    if text=='':\n",
    "      return 0.0\n",
    "    else:\n",
    "      prepare_text = self.prepare(text)\n",
    "      if len(prepare_text)==0:\n",
    "        return 0.0\n",
    "      else:\n",
    "        for word in prepare_text:\n",
    "          if word in self.norm_dict.keys() and self.norm_dict[word] in self.word_dict.keys():\n",
    "            lem_text.append(self.word_dict[self.norm_dict[word]])\n",
    "          else:\n",
    "              lem_text.append(0.0)\n",
    "\n",
    "      return np.mean(lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bb9c0-1700-4147-98bf-5ac2edcb8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.description = movies.description.fillna('')\n",
    "description = movies.loc[train_id, 'description'].tolist()\n",
    "lematizer = MyLemmatizer()\n",
    "lematizer.fit(description)\n",
    "movies.description = [lematizer.transform(text) for text in tqdm(movies.description, desc = '     DESCRIPTION')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38591a7-553b-4297-8f84-4e2f84499066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attr_count:\n",
    "  def __init__(self):\n",
    "    self.items_dict = {}\n",
    "\n",
    "  def fit(self, items:list):\n",
    "    items = list(chain.from_iterable(items))\n",
    "    c = Counter(items)\n",
    "    self.items_dict = {k:v/max(c.values()) for k, v in c.items()}\n",
    "\n",
    "  def transform(self, items:list):\n",
    "    if len(items) == 0:\n",
    "      return 0.0\n",
    "    else:\n",
    "      return np.mean([self.items_dict[item] if item in self.items_dict.keys() else 0.0 for item in items])\n",
    "\n",
    "\n",
    "def data_prepare(user_id:int):\n",
    "  global log\n",
    "  scaler = MinMaxScaler()\n",
    "  movies_list = list(set(log.loc[log.user_id==user_id, 'movie_id'].tolist()))\n",
    "  train_id, test_id = train_test_split(movies_list, test_size=0.2)\n",
    "\n",
    "  print('        LOAD')\n",
    "  movies = pd.read_csv('/content/movies.csv')\n",
    "  # output.clear(wait=True)\n",
    "  # print(print_text:= '✓ LOAD')\n",
    "\n",
    "  print('       DATA PREPARE:\\n')\n",
    "  movies.year = pd.to_datetime(movies.year, format='%Y-%m-%d')\n",
    "  movies.year = [date.timestamp() for date in tqdm(movies.year, desc = '            YEAR')]\n",
    "\n",
    "  movies.description = movies.description.fillna('')\n",
    "  description = movies.loc[train_id, 'description'].tolist()\n",
    "  lematizer = MyLemmatizer()\n",
    "  lematizer.fit(description)\n",
    "  movies.description = [lematizer.transform(text) for text in tqdm(movies.description, desc = '     DESCRIPTION')]\n",
    "\n",
    "  movies['genres'] = movies['genres'].apply(eval)\n",
    "  ac = attr_count()\n",
    "  ac.fit(movies.loc[train_id, 'genres'].tolist())\n",
    "  movies.genres = [ac.transform(g) for g in tqdm(movies.genres, desc = '          GENRES')]\n",
    "\n",
    "  movies['countries'] = movies['countries'].apply(eval)\n",
    "  ac = attr_count()\n",
    "  ac.fit(movies.loc[train_id, 'countries'].tolist())\n",
    "  movies['countries'] = [ac.transform(c) for c in tqdm(movies['countries'], desc = '       COUNTRIES')]\n",
    "\n",
    "  movies['staff'] = movies['staff'].apply(eval)\n",
    "  ac = attr_count()\n",
    "  ac.fit(movies.loc[train_id, 'staff'].tolist())\n",
    "  movies['staff'] = [ac.transform(s) for s in tqdm(movies['staff'], desc = '           STAFF')]\n",
    "\n",
    "  movies = movies.drop(['id', 'name', 'date_publication', 'year'], axis=1)\n",
    "  # output.clear(wait=True)\n",
    "  # print(print_text:= print_text+'\\n✓ DATA PREPARE')\n",
    "\n",
    "  print('       TRANSFORM TO TENSOR')\n",
    "  x_train = torch.tensor(movies.values, dtype=torch.float32)\n",
    "  user_log:pd.DataFrame = log.loc[log.user_id == user_id]\n",
    "  y_train = [[sum(user_log.loc[user_log.movie_id == m_id, 'duration'])] if m_id in train_id else [0.0] for m_id in movies.index.tolist()]\n",
    "  y_train = scaler.fit_transform(y_train)\n",
    "  y_train = torch.tensor(y_train, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "  x_test = torch.tensor(movies.values, dtype=torch.float32)\n",
    "  y_test = test_id\n",
    "  # output.clear(wait=True)\n",
    "  # print(print_text:= print_text+'\\n✓ DATA TRANSFORM TO TENSOR')\n",
    "\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68708449-3cf6-4dd2-889e-6342dfcccbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.colab import output\n",
    "from decimal import Decimal\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize, Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "stopwords = tuple(nltk_stopwords.words('russian'))\n",
    "\n",
    "!pip install torch torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
